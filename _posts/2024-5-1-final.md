---
layout: post
title: Blackjack
---

<link rel="stylesheet" type="text/css" href="./_style/style.css">

# Application of Deep Q-Networks in Blackjack

## Abstract

This project explores the application of a Deep Q-Network (DQN), a type of deep reinforcement learning, to the game of Blackjack. The objective was to develop an artificial intelligence agent capable of mastering Blackjack strategies to maximize winnings over time.  

## Introduction

### Purpose

Blackjack, also known as 21, is a popular card game in casinos worldwide. It presents a unique challenge to artificial intelligence due to its combination of easy-to-understand rules and complex strategic play based on incomplete information. This project aims to explore the application of Deep Q-Networks (DQN), a type of deep reinforcement learning algorithm, in devising optimal strategies for playing Blackjack. The objective is to train an intelligent agent that can make decisions that maximize potential earnings over many games, highlighting the practical application of DQN in environments with elements of both chance and skill.

### Brief overview of Blackjack Rules

<img src="https://lh7-us.googleusercontent.com/jCmogdeEoFOs-gxZo0q4vKFYyqPHiKIcDaHsMNLwgvzY3hL9LVavB2m24o6H7igdloL49T0W1xUYdIp2wybKJflIHn-bxgcoO80YAEID12cMHZAusQZC7itJp_I-PDVuniirr4CUVRTDvvG_R35Z1yLkQg=s2048" alt="img" style="zoom:50%;" />

- **Objective**: Aim to get a player's hand's total point value as close to 21 as possible without exceeding it.
- **Gameplay**: Both the player and the dealer start with two cards. The player's cards are usually visible, while the dealer has one card face up and one face down.
- **Player Decisions**: Players can choose to "Hit" (take additional cards) or "Stand" (keep their current hand).
- **Winning the Game**: At the end of the game, if a player's total exceeds 21, it's a "bust," and the player loses. Otherwise, the higher total between the player and the dealer wins.

## Methodology

### Environment

The environment specific to Blackjack implements the traditional rules of the game. It manages the cards, player actions, and the scoring system unique to Blackjack.

### State Representation

In this environment, the state is represented by the scores of the player's hand and the dealer's visible card. This representation is crucial as it provides the agent with the necessary information to make decisions based on the current game context.

### Action Management

The legal actions in Blackjack, namely "hit" and "stand," are managed by the environment, which presents these options to the agent based on the game's current state. This allows the agent to assess its strategy and decide on the next move.

### Reward Calculation

At the end of each game round, the environment calculates the results and issues rewards based on the outcomes—win, loss, or tie. These rewards are pivotal for the reinforcement learning process, as they provide feedback to the agent about the effectiveness of its decisions.

### Deep Q-Network Architecture

The Q-Network and the Target Network share an identical architecture but maintain separate parameters. The network consists of a series of fully connected layers with tanh activation functions to introduce non-linearity, allowing the model to learn more complex patterns in the data. The input to this network is the state representation of the Blackjack game, which includes numerical data encoding the player’s and dealer’s hand values.

- **Input Layer**: The input layer accepts the state of the game, which is a vector that encodes the current scores of the player and dealer.
- **Hidden Layers**: Multiple hidden layers are configured, where the size and number of these layers can be adjusted. Each layer uses a tanh activation function to provide non-linear transformation of inputs.
- **Output Layer**: The final layer is a linear layer that outputs the Q-values for each possible action in the given state. The number of actions in Blackjack is typically two: hit or stand.

### Training Process

The training of the DQN involves several key components and processes that enable the agent to learn effectively:

- **Replay Memory**: A crucial component of our DQN agent is the replay memory. It stores experiences observed during the interaction with the environment, which consists of tuples containing states, actions, rewards, subsequent states, and done signals. This approach mitigates the issues related to correlated data and non-stationary distributions.
- **Epsilon-Greedy Policy**: A policy commonly used in DQN training, the epsilon-greedy strategy, is employed to balance exploration and exploitation. This policy starts with a high likelihood of selecting random actions and gradually increases the propensity to take actions as suggested by the model, as epsilon decays.
- **Loss Calculation**: The loss is computed as the mean squared error between the predicted Q-values by the Q-Network for the actions taken and the target Q-values. The targets are calculated using the reward received for the action and the discounted highest Q-value of the next state as predicted by the Target Network, unless the episode has ended.
- **Optimizer**: The Adam optimizer is used for adjusting the weights of the neural network, selected for its effectiveness in handling sparse gradients and adaptive learning rate capabilities.

## Experiments and Results

The primary goal of our experiments was to evaluate the effectiveness of the Deep Q-Network (DQN) agent in playing Blackjack. To achieve this, we designed a series of experiments focused on measuring the performance of the agent over extensive simulations and under different game conditions.

### Training Setup

The DQN agent was trained over 10,000 episodes, with the architecture comprising a 3-layer multi-layer perceptron (MLP). Each episode consisted of a series of games where the agent utilized an epsilon-greedy policy for action selection. The epsilon value was set to decay linearly from 1.0 to 0.1 over the episodes to balance exploration with exploitation. The learning rate was fixed at 5e-5, and the discount factor for future rewards was set at 0.9.

### Evaluation Metrics

The agent's performance was assessed using the following metrics:
- **Win Rate**: The percentage of games won by the agent out of the total games played.
- **Average Reward**: The mean reward obtained by the agent per game, calculated over all games played.

### Results

<img src="https://lh7-us.googleusercontent.com/BbreMbSc8S7TKIEw8n9L9PxOj4SzuqEBGp0RM5wRWLLQosLnupVQnD4luAJNKfSNIAHuU5NCRGrMU4as1vqGJHy4jHqbGFWglFoJQKitme9ZBYfYE1m7he_2fjNCKKudnDFVYF78L22EHvhbQ5l6dykmCA=s2048" alt="img" style="zoom: 67%;" />

- **Win Rate and Rewards**: Throughout the 100,000 simulated games, the DQN agent achieved an average reward of -0.049 and a win rate of 42.14%. These metrics indicate that while the agent often outperformed random play, the challenging nature of Blackjack resulted in an overall loss across the games.
- **Reward Convergence**: The reward convergence graph indicates that the agent's reward stabilized below zero, which was expected due to the tough dynamics of Blackjack where the dealer has a structural advantage.

### Strategic Insights

Further tests were conducted to assess the impact of different numbers of decks on the agent's performance. The results suggest that fewer decks generally provide a slight advantage for strategies that rely on card counting, though our DQN agent did not explicitly employ such strategies.

<img src="https://lh7-us.googleusercontent.com/30eNon6Jg7P9aWK7Gpex_dyOaGW2VPgn4FOxuQw33xPgK_29BkMgcVzkpigQ3UynqsXIghSt0Ai1OZt7Re-E9wJl-f-4iIdBhiLJYJ2Q10Wk1lVJ9dtYSqIt5JAnYpZYaLLs_jNUfQ65OdseFqGNLM-q5w=s2048" alt="img" style="zoom: 33%;" /><img src="https://lh7-us.googleusercontent.com/eOmK5aYtjAL6y4KMUgLu_JyjQoLJbDx1j9RDNRn0CypeqZ-8l94Sc7jRMK6i_WJ0BsWoFto1PYF5GntAeiFh0Xm67bqFTZD_irLNMU2jtzDhKUmxtz-mc7ME9GNqea4KDwEcl8M78q0gRuamiPrxAW1FYg=s2048" alt="img" style="zoom: 33%;" /><img src="https://lh7-us.googleusercontent.com/rXlriHaC6umd-qsPfoLrUWzav9yezu_IA6F7Q6DG547idsjSeOhCB1TZIv_EJDPW2R3b9r_Y84WoN2e28BwYg-Jy0nyI9DzUNBIBs6s6ZtHEeqOd5NlraAmBosmUzpfHiRx2E5WeZfo3Dqi5ZyW2illddA=s2048" alt="img" style="zoom: 33%;" />

### DQN vs. Random Strategy

* Average Rewards of **DQN**: -0.0493
* Average Rewards of **Random**: -0.4116

The DQN agent's average reward of -0.0493 was significantly higher than that of the random strategy, which had an average reward of -0.4116. This comparison underscores the DQN's ability to learn and make informed decisions based on the game state, as opposed to making uninformed random choices.

The experiments conducted illustrate the potential and limitations of using a DQN agent for Blackjack. While the agent did not consistently win—reflected in the average negative rewards—it nonetheless demonstrated the capability to perform better than uninformed random strategies. 

## Conclusion

The project applies a Deep Q-Network (DQN) to solve Blackjack, aiming to explore the capabilities of reinforcement learning in a game dominated by both stochastic elements and strategic depth. The DQN agent demonstrated significant learning progress throughout the experiment, as evidenced by increasing win rates and average rewards over time. These results underscore the potential of DQN to develop nuanced strategies that can outperform traditional methods in complex decision environments.